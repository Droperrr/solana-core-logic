#!/usr/bin/env python3
"""
üß† –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π - –æ—Ç–ª–∞–¥–æ—á–Ω–∞—è –≤–µ—Ä—Å–∏—è
"""

import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
DB_PATH = 'db/solana_db.sqlite'
TOKEN_ADDRESSES = [
    'AL2HhMQLkJqeeK5w4akoogzyYBZ6GYkBfxjscCf2L2yC',
    'AGmXzgFQw8bLtsJvr5dgnHtyP6rP6GLwmZwgVqjFL6Q8'
]

def test_database_connection():
    """–¢–µ—Å—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
    print("üîß –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–û–î–ö–õ–Æ–ß–ï–ù–ò–Ø –ö –ë–î")
    try:
        conn = sqlite3.connect(DB_PATH)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–∞–±–ª–∏—Ü—ã
        tables = conn.execute("SELECT name FROM sqlite_master WHERE type='table'").fetchall()
        print(f"   ‚úÖ –¢–∞–±–ª–∏—Ü—ã –≤ –ë–î: {[t[0] for t in tables]}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π
        for token in TOKEN_ADDRESSES:
            count = conn.execute("SELECT COUNT(*) FROM transactions WHERE source_query_address = ?", (token,)).fetchone()[0]
            print(f"   üìä {token[:20]}...: {count:,} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π")
        
        conn.close()
        return True
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")
        return False

def get_sample_data(token_address, limit=10):
    """–ü–æ–ª—É—á–∏—Ç—å –æ–±—Ä–∞–∑–µ—Ü –¥–∞–Ω–Ω—ã—Ö"""
    conn = sqlite3.connect(DB_PATH)
    
    query = """
    SELECT 
        signature,
        block_time,
        fee_payer,
        transaction_type,
        datetime(block_time, 'unixepoch') as readable_time
    FROM transactions 
    WHERE source_query_address = ?
    ORDER BY block_time
    LIMIT ?
    """
    
    df = pd.read_sql_query(query, conn, params=[token_address, limit])
    conn.close()
    return df

def analyze_basic_patterns(token_address):
    """–ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
    print(f"\nüîç –ê–ù–ê–õ–ò–ó –¢–û–ö–ï–ù–ê: {token_address[:25]}...")
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –æ–±—Ä–∞–∑–µ—Ü –¥–∞–Ω–Ω—ã—Ö
        sample_df = get_sample_data(token_address, 100)
        print(f"   üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(sample_df)}")
        
        if len(sample_df) == 0:
            print("   ‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
            return None
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –∑–∞–ø–∏—Å–∏
        print(f"   üìã –ü–µ—Ä–≤—ã–µ –∑–∞–ø–∏—Å–∏:")
        for i, row in sample_df.head(3).iterrows():
            print(f"      {row['readable_time']} | {row['fee_payer'][:15] if row['fee_payer'] else 'Unknown'}... | {row['transaction_type']}")
        
        # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        unique_wallets = sample_df['fee_payer'].nunique()
        date_range = sample_df['readable_time'].nunique()
        
        print(f"   üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (–æ–±—Ä–∞–∑–µ—Ü):")
        print(f"      –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ—à–µ–ª—å–∫–æ–≤: {unique_wallets}")
        print(f"      –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: {sample_df['readable_time'].min()} - {sample_df['readable_time'].max()}")
        
        # –ü–æ–∏—Å–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        sample_df['datetime'] = pd.to_datetime(sample_df['readable_time'])
        sample_df['time_diff'] = sample_df['datetime'].diff().dt.total_seconds()
        
        rapid_transactions = sample_df[sample_df['time_diff'] < 5]
        print(f"      –ë—ã—Å—Ç—Ä—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π (<5 —Å–µ–∫): {len(rapid_transactions)} –∏–∑ {len(sample_df)}")
        
        if len(rapid_transactions) > 0:
            print(f"      –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª: {sample_df['time_diff'].min():.1f} —Å–µ–∫")
            print(f"      –ú–µ–¥–∏–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª: {sample_df['time_diff'].median():.1f} —Å–µ–∫")
        
        return {
            'token': token_address,
            'sample_size': len(sample_df),
            'unique_wallets': unique_wallets,
            'rapid_count': len(rapid_transactions),
            'median_interval': sample_df['time_diff'].median(),
            'first_time': sample_df['readable_time'].min(),
            'last_time': sample_df['readable_time'].max()
        }
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        return None

def compare_tokens(results):
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏"""
    print(f"\nüîÑ –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó")
    print("=" * 40)
    
    if len(results) < 2:
        print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
        return
    
    print(f"üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤:")
    
    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –º–µ–¥–∏–∞–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
    intervals = [r['median_interval'] for r in results if r['median_interval'] is not None]
    
    if len(intervals) >= 2:
        print(f"   ‚è±Ô∏è –ú–µ–¥–∏–∞–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã:")
        for i, result in enumerate(results):
            if result['median_interval'] is not None:
                print(f"      –¢–æ–∫–µ–Ω {i+1}: {result['median_interval']:.1f} —Å–µ–∫")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
        if len(intervals) == 2:
            diff = abs(intervals[0] - intervals[1])
            print(f"   üìä –†–∞–∑–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤: {diff:.1f} —Å–µ–∫")
            
            if diff < 2.0:
                print(f"   ‚úÖ –°–ò–ì–ù–ê–õ –ö–û–û–†–î–ò–ù–ê–¶–ò–ò: –°—Ö–æ–∂–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã!")
            else:
                print(f"   ‚ùå –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è —Ä–∞–∑–Ω–æ—Å—Ç—å –≤ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞—Ö")
    
    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç –±—ã—Å—Ç—Ä—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
    rapid_percentages = []
    for result in results:
        if result['sample_size'] > 0:
            rapid_pct = result['rapid_count'] / result['sample_size'] * 100
            rapid_percentages.append(rapid_pct)
            print(f"   üöÄ –ë—ã—Å—Ç—Ä—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π: {rapid_pct:.1f}%")
    
    if len(rapid_percentages) >= 2:
        rapid_diff = abs(rapid_percentages[0] - rapid_percentages[1])
        print(f"   üìä –†–∞–∑–Ω–æ—Å—Ç—å %% –±—ã—Å—Ç—Ä—ã—Ö: {rapid_diff:.1f}%")
        
        if rapid_diff < 10.0:
            print(f"   ‚úÖ –°–ò–ì–ù–ê–õ –ö–û–û–†–î–ò–ù–ê–¶–ò–ò: –°—Ö–æ–∂–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –±—ã—Å—Ç—Ä—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π!")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üß† –ê–ù–ê–õ–ò–ó –ü–ê–¢–¢–ï–†–ù–û–í –¢–†–ê–ù–ó–ê–ö–¶–ò–ô - –û–¢–õ–ê–î–û–ß–ù–ê–Ø –í–ï–†–°–ò–Ø")
    print("=" * 60)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
    if not test_database_connection():
        return
    
    print(f"\nüìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {len(TOKEN_ADDRESSES)} —Ç–æ–∫–µ–Ω–æ–≤")
    
    results = []
    
    # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
    for token_address in TOKEN_ADDRESSES:
        result = analyze_basic_patterns(token_address)
        if result:
            results.append(result)
    
    # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    compare_tokens(results)
    
    print(f"\nüéâ –û–¢–õ–ê–î–û–ß–ù–´–ô –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù")
    print(f"üìù –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")

if __name__ == "__main__":
    main() 