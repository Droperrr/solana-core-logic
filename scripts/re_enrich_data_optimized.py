#!/usr/bin/env python3
"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö (Re-Enrichment Pipeline)
–†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã:
1. –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –ë–î
2. –ú–µ–¥–ª–µ–Ω–Ω–æ–π –∑–∞–ø–∏—Å–∏ (–ø–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)
3. –ë–ª–æ–∫–∏—Ä–æ–≤–æ–∫ SQLite
"""

import argparse
import json
import sqlite3
import sys
import os
import time
from pathlib import Path
from typing import List, Optional, Dict, Any
import logging
from contextlib import contextmanager

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent.parent))

from decoder.decoder import TransactionDecoder

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

CURRENT_PARSER_VERSION = "2.1.0"
DEFAULT_BATCH_SIZE = 1000  # –£–≤–µ–ª–∏—á–µ–Ω —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
MAX_RETRIES = 3
RETRY_DELAY = 0.5

@contextmanager
def get_optimized_db_connection(db_path: str):
    """
    –°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å SQLite –¥–ª—è –º–∞—Å—Å–æ–≤—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
    """
    conn = sqlite3.connect(db_path, timeout=30.0)
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º–∞—Å—Å–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π
    conn.execute("PRAGMA journal_mode=WAL")  # Write-Ahead Logging
    conn.execute("PRAGMA synchronous=NORMAL")  # –ë—ã—Å—Ç—Ä–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
    conn.execute("PRAGMA cache_size=10000")    # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –∫—ç—à
    conn.execute("PRAGMA temp_store=MEMORY")   # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã –≤ –ø–∞–º—è—Ç–∏
    conn.execute("PRAGMA mmap_size=268435456") # 256MB memory mapping
    
    try:
        yield conn
    finally:
        conn.close()

def get_transactions_for_reprocessing(
    conn: sqlite3.Connection,
    signatures: Optional[List[str]] = None,
    parser_version_from: Optional[str] = None,
    limit: Optional[int] = None
) -> List[dict]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –¥–ª—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    """
    cursor = conn.cursor()
    
    if signatures:
        placeholders = ','.join(['?' for _ in signatures])
        query = f"""
            SELECT signature, raw_json, parser_version, source_query_type, source_query_address
            FROM transactions 
            WHERE signature IN ({placeholders})
            ORDER BY block_time DESC
        """
        params = signatures
    else:
        if parser_version_from:
            query = """
                SELECT signature, raw_json, parser_version, source_query_type, source_query_address
                FROM transactions 
                WHERE parser_version <= ?
                ORDER BY block_time DESC
            """
            params = [parser_version_from]
        else:
            query = """
                SELECT signature, raw_json, parser_version, source_query_type, source_query_address
                FROM transactions 
                ORDER BY block_time DESC
            """
            params = []
    
    if limit:
        query += f" LIMIT {limit}"
    
    cursor.execute(query, params)
    results = cursor.fetchall()
    
    transactions = []
    for row in results:
        signature, raw_json_str, current_version, source_query_type, source_query_address = row
        try:
            raw_json = json.loads(raw_json_str) if raw_json_str else {}
            transactions.append({
                'signature': signature,
                'raw_json': raw_json,
                'current_parser_version': current_version,
                'original_source_query_type': source_query_type,
                'original_source_query_address': source_query_address
            })
        except json.JSONDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON –¥–ª—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ {signature}: {e}")
            continue
    
    return transactions

def reprocess_batch_optimized(
    decoder: TransactionDecoder,
    conn: sqlite3.Connection,
    batch: List[dict],
    dry_run: bool = False
) -> Dict[str, int]:
    """
    –ü–µ—Ä–µ–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–∞–∫–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
    
    Returns:
        Dict —Å –∫–ª—é—á–∞–º–∏ 'success', 'errors'
    """
    if dry_run:
        logger.info(f"[DRY RUN] –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞ –∏–∑ {len(batch)} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π")
        return {'success': len(batch), 'errors': 0}
    
    cursor = conn.cursor()
    success_count = 0
    error_count = 0
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–∞—Å—Å–æ–≤–æ–π –≤—Å—Ç–∞–≤–∫–∏
    updates = []
    
    for transaction_data in batch:
        signature = transaction_data['signature']
        raw_json = transaction_data['raw_json']
        current_version = transaction_data['current_parser_version']
        
        try:
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é
            decoded_result = decoder.decode_transaction(raw_json)
            
            if decoded_result is None:
                logger.warning(f"–î–µ–∫–æ–¥–µ—Ä –≤–µ—Ä–Ω—É–ª None –¥–ª—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ {signature}")
                error_count += 1
                continue
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
            original_source_type = transaction_data.get('original_source_query_type', 'reprocessing')
            original_source_address = transaction_data.get('original_source_query_address', 'batch')
            
            enriched_data_str = json.dumps(decoded_result, ensure_ascii=False)
            
            updates.append((
                enriched_data_str,
                CURRENT_PARSER_VERSION,
                signature
            ))
            
            success_count += 1
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ {signature}: {e}")
            error_count += 1
    
    # –ú–∞—Å—Å–æ–≤–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
    if updates:
        try:
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π UPDATE –∑–∞–ø—Ä–æ—Å
            cursor.executemany("""
                UPDATE transactions 
                SET enriched_data = ?, 
                    parser_version = ?, 
                    updated_at = CURRENT_TIMESTAMP
                WHERE signature = ?
            """, updates)
            
            logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–æ {len(updates)} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –≤ –ø–∞–∫–µ—Ç–µ")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –º–∞—Å—Å–æ–≤–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {e}")
            error_count += len(updates)
            success_count = 0
    
    return {'success': success_count, 'errors': error_count}

def wait_for_db_unlock(db_path: str, max_wait: int = 30) -> bool:
    """
    –û–∂–∏–¥–∞–µ—Ç –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    """
    for attempt in range(max_wait):
        try:
            with sqlite3.connect(db_path, timeout=1.0) as test_conn:
                test_conn.execute("SELECT 1").fetchone()
                return True
        except sqlite3.OperationalError as e:
            if "database is locked" in str(e):
                logger.info(f"–ë–î –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞, –æ–∂–∏–¥–∞–Ω–∏–µ... ({attempt+1}/{max_wait})")
                time.sleep(1)
            else:
                raise
    return False

def main():
    parser = argparse.ArgumentParser(description="–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π")
    
    # –û–ø—Ü–∏–∏ –¥–ª—è –≤—ã–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--all', action='store_true', 
                      help='–ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏')
    group.add_argument('--signatures-file', type=str,
                      help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å–æ —Å–ø–∏—Å–∫–æ–º –ø–æ–¥–ø–∏—Å–µ–π –¥–ª—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏')
    group.add_argument('--parser-version-from', type=str,
                      help='–ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ —Å –≤–µ—Ä—Å–∏–µ–π –ø–∞—Ä—Å–µ—Ä–∞ –Ω–µ –≤—ã—à–µ —É–∫–∞–∑–∞–Ω–Ω–æ–π')
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ü–∏–∏
    parser.add_argument('--limit', type=int, default=None,
                       help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏')
    parser.add_argument('--dry-run', action='store_true',
                       help='–†–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ –∑–∞–ø–∏—Å–∏ –≤ –ë–î')
    parser.add_argument('--batch-size', type=int, default=DEFAULT_BATCH_SIZE,
                       help=f'–†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è –º–∞—Å—Å–æ–≤—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {DEFAULT_BATCH_SIZE})')
    parser.add_argument('--db-path', type=str, default='db/solana_db.sqlite',
                       help='–ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö')
    parser.add_argument('--wait-for-unlock', action='store_true',
                       help='–û–∂–∏–¥–∞—Ç—å –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ë–î')
    
    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ë–î
    if not os.path.exists(args.db_path):
        logger.error(f"–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {args.db_path}")
        sys.exit(1)
    
    # –û–∂–∏–¥–∞–µ–º –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ë–î –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if args.wait_for_unlock:
        if not wait_for_db_unlock(args.db_path):
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–∂–¥–∞—Ç—å—Å—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ë–î")
            sys.exit(1)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–µ–∫–æ–¥–µ—Ä
    logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–µ–∫–æ–¥–µ—Ä–∞...")
    from decoder.enrichers.net_token_change_enricher import NetTokenChangeEnricher
    from decoder.enrichers.compute_unit_enricher import ComputeUnitEnricher  
    from decoder.enrichers.sequence_enricher import SequenceEnricher
    
    decoder = TransactionDecoder(enrichers=[
        NetTokenChangeEnricher(),
        ComputeUnitEnricher(),
        SequenceEnricher()
    ])
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
    with get_optimized_db_connection(args.db_path) as conn:
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if args.signatures_file:
                with open(args.signatures_file, 'r') as f:
                    signatures = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                if not signatures:
                    logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–¥–ø–∏—Å–∏ –∏–∑ —Ñ–∞–π–ª–∞")
                    sys.exit(1)
                transactions = get_transactions_for_reprocessing(conn, signatures=signatures)
            elif args.parser_version_from:
                transactions = get_transactions_for_reprocessing(
                    conn, parser_version_from=args.parser_version_from, limit=args.limit
                )
            else:  # --all
                transactions = get_transactions_for_reprocessing(conn, limit=args.limit)
            
            if not transactions:
                logger.info("–ù–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –¥–ª—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏")
                return
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(transactions)} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –¥–ª—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏")
            logger.info(f"–†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞: {args.batch_size}")
            
            if args.dry_run:
                logger.info("üß™ –†–ï–ñ–ò–ú –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø - –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –ë–î –Ω–µ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞–∫–µ—Ç–∞–º–∏
            total_success = 0
            total_errors = 0
            
            for i in range(0, len(transactions), args.batch_size):
                batch = transactions[i:i + args.batch_size]
                batch_num = i // args.batch_size + 1
                total_batches = (len(transactions) + args.batch_size - 1) // args.batch_size
                
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞ {batch_num}/{total_batches} ({len(batch)} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π)")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞–∫–µ—Ç
                results = reprocess_batch_optimized(decoder, conn, batch, args.dry_run)
                
                total_success += results['success']
                total_errors += results['errors']
                
                # –ö–æ–º–º–∏—Ç–∏–º –ø–∞–∫–µ—Ç (–≤–º–µ—Å—Ç–æ –∫–∞–∂–¥–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏)
                if not args.dry_run:
                    conn.commit()
                
                # –ü—Ä–æ–≥—Ä–µ—Å—Å
                processed = min(i + args.batch_size, len(transactions))
                logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {processed}/{len(transactions)} ({processed/len(transactions)*100:.1f}%)")
            
            # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
            logger.info(f"""
üéâ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ü–ï–†–ï–û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê:
‚úÖ –£—Å–ø–µ—à–Ω–æ: {total_success}
‚ùå –û—à–∏–±–æ–∫: {total_errors}
üìä –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {total_success/(total_success+total_errors)*100:.1f}%
‚ö° –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã: WAL —Ä–µ–∂–∏–º, –ø–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞, —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –∫—ç—à
            """)
            
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            sys.exit(1)

if __name__ == "__main__":
    main() 